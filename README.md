# ANN-with-Sigmoid-/-RELU-Activation-Functions
Sigmoid activation function accepts positive values and provides the output in the range of 0 and 1.
RELU only accepts positive values and returns zero if value is negative.
simple example of ANN with both sigmoid and RELU activation functions, implemented with a single layer as well as multiple layers. 
